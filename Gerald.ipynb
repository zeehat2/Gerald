{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python2","display_name":"Python 2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15+"},"colab":{"name":"Gerald.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"cCrjlUiOylNR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"634f6e98-f81e-40b4-f547-6ac28e059062","executionInfo":{"status":"ok","timestamp":1571618857648,"user_tz":240,"elapsed":521,"user":{"displayName":"Samon Gervais","photoUrl":"","userId":"06411007464490543392"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FAr7iclhrqYg","colab_type":"text"},"source":["Please excuse the formatting, haven't really done much markdown before. Anyway, we get started by importing some libraries, and loading in our data.\n"]},{"cell_type":"code","metadata":{"id":"VIkNy1gGrqYn","colab_type":"code","colab":{}},"source":["import json\n","import re\n","import numpy as np    \n","import sklearn as sk\n","import scipy as sp\n","from sklearn import decomposition\n","from nltk import PorterStemmer\n","from scipy import sparse\n","from sklearn.naive_bayes import BernoulliNB"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aySnnB6GrqY0","colab_type":"code","colab":{}},"source":["\n","with open(\"drive/My Drive/gerald_data/train.json\") as file:\n","  train_data = json.load(file)[::10]\n","with open(\"drive/My Drive/gerald_data/test.json\") as file:\n","  test_data = json.load(file)[::10]\n","with open(\"drive/My Drive/gerald_data/stopwords.txt\") as file:\n","  stopwords = file.read().splitlines()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oI8EqQXRrqY6","colab_type":"text"},"source":["# Preprocessing\n","\n","Like in assignment 2, lowercase this bad boy and remove the punctuation. Then remove stopwords and apply stemming."]},{"cell_type":"code","metadata":{"id":"4jbCEZSTrqY9","colab_type":"code","colab":{}},"source":["ps = PorterStemmer()\n","def preprocess(post):\n","  words = re.sub('[^a-z ]', '', post[\"post\"].lower()).split()\n","  return map(ps.stem, filter(lambda w : w not in stopwords, words))\n","\n","posts = [preprocess(post) for post in train_data]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivKNSI03rqZD","colab_type":"text"},"source":["Now we create our list of unique words. However, we also want to discard the nonsense, so we delete words from the list that occur in too few posts. "]},{"cell_type":"code","metadata":{"id":"sd7OPXiWrqZF","colab_type":"code","colab":{}},"source":["TOO_RARE_THRESHOLD = 0.0003  # Rate of occurence below which a word is too rare to be considered\n","\n","words = dict()  # Match words to the number of posts they occured in.\n","for post in posts:\n","    for word in list(set(post)): # set trick removes duplicates\n","       if word not in occured:\n","            if word in words:\n","                words[word] += 1\n","            else:\n","                words[word] = 1\n","\n","print \"Features before removal of rare words: %d\" % len(words)                \n","for word in words.keys():\n","    if words[word] < TOO_RARE_THRESHOLD * len(posts):\n","        del(words[word])\n","words = sorted(words.keys(), key=words.get, reverse=True)    # words is now a list, sorted by how many times each word occured.    \n","        \n","print \"After removal: %d\" % len(words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1sJGmKtmkUOg","colab_type":"text"},"source":["Next we define the functions we will use to vectorize our data, which we can then feed into some models!"]},{"cell_type":"code","metadata":{"id":"1tFij0Vwq9Ae","colab_type":"code","colab":{}},"source":["def vectorize_x(data):\n","  X = sp.sparse.lil_matrix((len(data), len(words)), dtype=np.int8)\n","  for i, post in enumerate(data):\n","    for word in list(set(post)):\n","      try: \n","        X.rows[i].append(words.index(word)) # In a lil_matrix, there is self.rows, where the ith element is a list of column indexes where there are non-zero elements...\n","        X.data[i].append(1)                 # ...and self.data, where the ith element is a list of the non-zero elements that occur in that row.\n","      except: pass\n","    if i % 1000 == 0:\n","      print i\n","  return X.tocsr()\n","  \n","\n","def vectorize_gender(data):\n","  return np.array(map(lambda post : 1 if post[\"gender\"] == \"male\" else 0, data)) # Change genders to 1:male, 0:female\n","\n","def vectorize_age(data):\n","  return np.array((post['age'] for post in data))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6IWtfjMolc7c","colab_type":"text"},"source":["Here we apply SVD to reduce the dimensionality of the data to something more managable."]},{"cell_type":"code","metadata":{"id":"2KM3HPnqeN6l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"0384593d-2c1c-4d0e-9afe-bcb1a2afcff7","executionInfo":{"status":"ok","timestamp":1571628370090,"user_tz":240,"elapsed":595590,"user":{"displayName":"Samon Gervais","photoUrl":"","userId":"06411007464490543392"}}},"source":["N_COMPONENTS = 300\n","\n","svd = sk.decomposition.TruncatedSVD(N_COMPONENTS)\n","X_train = svd.fit_transform(vectorize_x(posts)) \n","X_test = svd.transform(vectorize_x([preprocess(post) for post in test_data]))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n","14000\n","15000\n","16000\n","17000\n","18000\n","19000\n","20000\n","21000\n","22000\n","23000\n","24000\n","25000\n","26000\n","27000\n","28000\n","29000\n","30000\n","31000\n","32000\n","33000\n","34000\n","35000\n","36000\n","37000\n","38000\n","39000\n","40000\n","41000\n","42000\n","43000\n","44000\n","45000\n","46000\n","47000\n","48000\n","49000\n","50000\n","51000\n","52000\n","0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n","13000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OFmY4SanmNgy","colab_type":"text"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"6dX0V0JYmvbB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"47b6c888-0eac-4325-9589-c49364b1c99d"},"source":["Y_gender_train = vectorize_gender(train_data)\n","Y_gender_test = vectorize_gender(test_data)\n","\n","bayes = BernoulliNB()\n","bayes.fit(X_train, Y_gender_train)\n","print \"Naive bayes train accuracy: {:.2%}\".format(bayes.score(X_train, Y_gender_train))\n","print \"Naive bayes test accuracy: {:.2%}\".format(bayes.score(X_test, Y_gender_test))\n","\n","N_NEIGHBORS = 10\n","nn = sk.neighbors.KNeighborsClassifier(N_NEIGHBORS)\n","nn.fit(X_train, Y_gender_train)\n","print \"Nearest-neighbors train accuracy: {:.2%}\".format(nn.score(X_train, Y_gender_train))\n","print \"Nearest-neighbors test accuracy: {:.2%}\".format(nn.score(X_test, Y_gender_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Naive bayes train accuracy: 61.66%\n","Naive bayes test accuracy: 59.59%\n"],"name":"stdout"}]}]}